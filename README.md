0. 
- Sarah Choi (ssc185), Nirad Shah (ns1361), Kenneth Augusto (kma234)

1. 
- Everything seems like it's working fine. We put our error testing at the end of the script (2 statements testing incorrect type entries and 1 statement testing the absence of a required field/primary key)

2. 
- Collaborated with Nirad and Kenneth. 
- In order to copy the csv into the table, we referenced the following site: https://www.postgresqltutorial.com/postgresql-tutorial/import-csv-file-into-posgresql-table/. We used this to understand the basic procedure and then we designed our own table to copy it into. (We labeled columns and assigned what we believed to be appropriate variable types for each column). 
- To learn how to create new primary keys (not specified in csv) we referenced: https://neon.tech/postgresql/postgresql-tutorial/postgresql-primary-key
- To learn how to alter a table to add a column: https://www.postgresql.org/docs/current/sql-altertable.html
- Used https://www.postgresql.org/docs/current/sql-insert.html to figure out how to not enter the same key twice when importing data from two different columns into the same table (such as the applicant ethnicity and coapplicant ethnicity). Explained better in #4
- We all did preliminary.sql. Together, we all decided how to put the tables into 3nf form, dividing the columns into separate tables and deciding how we would connect the tables using foreign keys and primary keys. Sarah made the create table statements. 
- Sarah did the insert statements for ethnicity, racecode, denialcode, application, applicant race, coapplicant race, denialreasons
- Nirad did the insert statements for sex, agency, purchasertype, propertytype, loantype, loanpurpose, actiontaken, null table
- Kenneth did the insert statements for state, county, msamd, preapproval, owner occupancy, hoepa status, lien status, location
- In order to reconstruct the CSV at the end, we referenced different sites including https://stackoverflow.com/questions/1241178/pivot-on-multiple-columns-using-pivot-operator to show us how to pivot data and taught us CASE WHEN, and then used https://medium.com/@jan_hkmn/advanced-pivoting-in-sql-the-max-case-trick-50d58869fc96 to make up our final columns. We needed to use MAX(CASE WHEN...) to fill the row giving us our pivot table.
- We referenced https://www.atlassian.com/data/sql/how-to-export-data-to-csv-or-excel to learn how to output data into csvs.
- We also referenced  https://www.postgresql.org/docs/current/sql-copy.html to copy csv into a table.
- This reference https://stackoverflow.com/questions/45892420/postgresql-copy-empty-string-as-null-not-work is how to prevent null values from being inputted.

3. 
- What is the minimum information would you need to collect from a user to allow
them to submit a mortgage application? What types of (multiple choice) questions
would you ask? Which columns do you think are genrated by the bank vs collected
from the user?

- Minimum Information 
The bare minimum information we need to collect from a user to allow them to submit a mortgage application is:
Loan Amount
Loan Purpose
Loan Type
Property Type
Owner Occupancy
Applicant Income
State Name
County Name

This data would give the bank key information about the type of loan the user is requesting, the details on the property involved, 
and if the applicant’s income is sufficient to pay off the loan. 

- Types of Questions
We would ask the users mainly multiple choice questions that are either categories or ranges. For example, if the user is selecting the loan purpose,
we would provide the 3 categories (Home purchase, Home improvement, Refinancing). And if the user was selecting something like Applicant Income, 
we might have ranges of salary in increments of $25,000. Having pre-defined choices/ranges makes it faster for the user to interact with, 
so apart from the loan amount, we plan to keep it mostly multiple choice questions.

- User vs Bank Generated Columns

Columns generated by the user include loan amount, loan purpose, loan type, applicant income, applicant sex, applicant race, applicant ethnicity,
co applicant sex, co applicant race, co applicant ethnicity, property type, owner occupancy, and potentially pre approval.
Additionally, state name and county name can be collected from the user and the information from these choices will help the bank generate other
geographic/demographic data for the application. 

The loan amount and applicant income would have to be modified in the database so that they only store the value in the thousands. 
Also, the columns that contain codes related to the name columns would be populated by “the bank” or the system because the system should not be asking any questions about the codes to the user.

The columns generated by the bank will include as of year, respondent id (the id that they choose to assign to the applicant), agency, action taken,
purchaser type, denial reasons, rate spread, hoepa status, lien status, edit status, sequence number, and application data indicator. 
Additionally, census tract number, msamd, population, minority population, hud median family income, tract to msamd income, 
number of owner occupied units, and number of 1 to 4 family units can be generated by the bank based on the county provided by the user.


4. (problems, how long)
When using the COPY function to import the data from the csv file into the table, we had a syntax error for the INTEGER columns because any empty cell ('') was considered a string, not a null cell. To fix this, we looked at the COPY documentation (https://www.postgresql.org/docs/17/sql-copy.html) and found the FORCE_NULL and NULL sections to make any empty cells into the NULL datatype for the INTEGER columns.
We weren't sure how to create a new unique primary key for applicant_id for example. When we looked it up, we found the option to include SERIAL to generate a unique integer id: https://neon.tech/postgresql/postgresql-tutorial/postgresql-primary-key
When trying to insert values into the smaller tables (for example ethnicity code and ethnicity name), we selected distinct values from the applicant ethnicities and after selected distinct values from the coapplicant ethnicities. However, because the ethnicity code was a primary key, we had trouble trying to insert duplicate ethnicities. We used the 'ON CONFLICT DO NOTHING' that we found in the postgresql documentation (https://www.postgresql.org/docs/current/sql-insert.html).
We couldn't figure out how to link the preliminary table to the race column so we temporarily added an id column to preliminary: https://www.postgresql.org/docs/current/sql-altertable.html.
So far, we've taken 11 hours.
It was a lot of difficulty figuring out how to reconstruct the tables into one CSV. Our line to do it is very long and tedious and we had to reference different YouTube videos and online forums to troubleshoot the problems we had.
We also had a lot of trouble picking the schema and how all the tables were connect. We had to rewrite the connections several times during the course of the project as a result of error codes that we would run into when trying to reference another table.
